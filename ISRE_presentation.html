<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Accuracy of Automatic Emotion Recognition from Voice</title>
    <meta charset="utf-8" />
    <meta name="author" content="Damien Dupré" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Accuracy of Automatic Emotion Recognition from Voice
## International Society for Research on Emotion
### Damien Dupré
### Amsterdam, July 10th 2019

---





# Automatic Emotion Recognition

Since the 70's, automatic systems has been develloped to recognize individual emotions.

With the advancement of Machine Learning technics, multiple companies are now providing solution for automatic emotion recognition for diverse applications such as marketing, automotive, activity and helth care monitoring.

Automatic emotion recognition systems can use:
 - physiological sensors and brain activity measurements
 - textual expression for sentiment analysis
 - visual capture of facial expressions and postures
 - audio capture of vocal expressions

---

# Vocal Expressions of Emotions

Voice is one of the most important channel to communicate emotions not only thought the word used but also thought the tonality used to pronounce these words.

Several companies are providing emotion recognition systems from voice:

* AudEERING
* Affectiva
* Neurodata Labs
* Beyond Verbal
* Amazone Web Service
* Microsoft Azure
* Google Cloud
* ...

---
class: inverse, center, middle

# Method

---

# Vocal Expression of Emotions

In order to evaluate the accuracy to recognise emotion from voice tonality we have extracted the audio track from the videos of the GEMEP-CS database (Bänziger &amp; Scherer, 2010; Bänziger, Mortillaro &amp; Scherer, 2012).

The Geneva Multimodal Emotion Portrayals Core Set (GEMEP-CS) database is made of video recording of:
- 10 professional Frenchspeaking theater actors (5 females and 5 males, Mage = 37.1)
- they had to enact up to 15 emotion categories (facial expression, posture and voice)
- for each enactement they had to pronounce a pseudosentence that sounds like an unknown real language, consisting of meaningless words constituted by phonemes from several languages

Why GEMEP?
* language words and sentences can be emotionally tainted
* the way words are pronounce can biased by their emotional meaning
* pseudosentence aims to remove this potential bias

---

# Emotion Categories

|Key|Emotion                |Valence|Arousal|
|---|-----------------------|-------|-------|
|Amu|Amusement              |+      |+      |
|Pri|Pride                  |+      |+      |
|Joy|Elated Joy             |+      |+      |
|Rel|Relief                 |+      |-      |
|Int|Interest               |+      |-      |
|Ple|Pleasure               |+      |-      |
|Ang|Hot anger (rage)       |-      |+      |
|Fea|(Panic) Fear           |-      |+      |
|Des|Despair                |-      |+      |
|Irr|Irritation (cold anger)|-      |-      |
|Anx|Anxiety                |-      |-      |
|Sad|Sadness                |-      |-      |
|Adm|Admiration             |AE     |AE     |
|Amu|Amusement              |AE     |AE     |
|Ten|Tenderness             |AE     |AE     |
|Dis|Disgust                |AE     |AE     |
|Con|Contempt               |AE     |AE     |
|Sur|Surprise               |AE     |AE     |

*AE = Additional Emotion

---

# AudEERING Emotion Recognition from Voice

AudEERING is a German company founded in 2012 by Dagmar and Björn Schuller.

They provide openSMILE, an open-source system to perform audio feature spaces in real time as well as Emotion Recognition
https://github.com/naxingyu/opensmile

They also have developped SensAI, an API solution to analyse emotions from voice features (Eyben, Huber, Marchi, Schuller, &amp; Schuller, 2015).

More precisely SensAI can recognize:
1. Overall emotion
2. Probability of expressing emotion categories: passion, panic, nervousness, disgust, contentment, affection, fear, irritation, satisfaction, frustration, enthusiasm, worry, boredom, interest, tension, joy, depression, stress, pride, excitement, sadness, anger, relaxation, happiness
3. Probability of expressing emotion dimensions: valence, activation, dominance

---
class: inverse, center, middle

# Results

---

# Matching Categorical Emotion Expressed with Overall Emotion Recognized

Among the 24 emotions recognized by SensAI and the 15 expressed in the GEMEP only 10 are matching: disgust, contentment, fear, irritation, worry, interest, joy, pride, sadness, anger.



The proportion of correct overall recognition by SensAI among these emotions is 6.67%


&lt;img src="ISRE_presentation_files/figure-html/unnamed-chunk-1-1.png" style="display: block; margin: auto;" /&gt;

---

class: inverse, center, middle

# Thanks for your attention!

Damien Dupré

Dublin City University

damien.dupre@dcu.ie
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
