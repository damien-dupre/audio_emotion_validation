\documentclass[]{elsarticle} %review=doublespace preprint=single 5p=2 column
%%% Begin My package additions %%%%%%%%%%%%%%%%%%%
\usepackage[hyphens]{url}

  \journal{ISRE 2019} % Sets Journal name


\usepackage{lineno} % add
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\bibliographystyle{elsarticle-harv}
\biboptions{sort&compress} % For natbib
\usepackage{graphicx}
\usepackage{booktabs} % book-quality tables
%%%%%%%%%%%%%%%% end my additions to header

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \usepackage{fontspec}
  \ifxetex
    \usepackage{xltxtra,xunicode}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{€}
\fi
% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\ifxetex
  \usepackage[setpagesize=false, % page size defined by xetex
              unicode=false, % unicode breaks when used with xetex
              xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi
\hypersetup{breaklinks=true,
            bookmarks=true,
            pdfauthor={},
            pdftitle={Accuracy of automatic emotion recognition from voice},
            colorlinks=false,
            urlcolor=blue,
            linkcolor=magenta,
            pdfborder={0 0 0}}
\urlstyle{same}  % don't use monospace font for urls

\setcounter{secnumdepth}{0}
% Pandoc toggle for numbering sections (defaults to be off)
\setcounter{secnumdepth}{0}
% Pandoc header



\begin{document}
\begin{frontmatter}

  \title{Accuracy of automatic emotion recognition from voice}
    \author[University College Dublin]{Damien Dupré\corref{c1}}
   \ead{damien.dupre@ucd.ie} 
   \cortext[c1]{Corresponding Author}
    \author[Queen's University Belfast]{Gary McKeown}
   \ead{g.mckeown@qub.ac.uk} 
  
      \address[University College Dublin]{The Insight Centre for Data Analytics, University College Dublin,
Republic of Ireland}
    \address[Queen's University Belfast]{School of Psychology, Queen's University Belfast, United Kingdom}
  
  \begin{abstract}
  
  \end{abstract}
  
 \end{frontmatter}

Voice is one of the main communicative sources of evidence in
interpreting the expression of emotion (Poria, Cambria, Bajpai, \&
Hussain, 2017). Affective computing aims to create systems and
algorithms that automatically analyse people's emotional state.
Consequently, several companies such as Affectiva, Beyond Verbal and
Audeering have developed automatic systems to analyse the vocal
expression of emotions. However, little is known about the accuracy of
such systems.

To evaluate the accuracy of automatic emotion recognition from voice, we
processed vocal expressions from the GEMEP database with ``SensAI
Emotion'' developed by Audeering. The GEMEP database contains
audio-video recordings from 10 actors performing 17 different emotional
scenarios in which they vocalize the same non-words with an emotional
tonality (Bänziger \& Scherer, 2010).SensAI Emotion analyses emotions
from speech and renders a value for 23 affective states and for valence
and arousal dimensions (Eyben, Scherer, \& Schuller, 2018).

In terms of category recognition, the accuracy of SensAI labelling
GEMEPvocal expressions of emotion is 6.67\%. However, this low result is
partly due to the high number of different affective state labels
recognized. To bypass this label matching bias, we compared the
recognition accuracy for valence and arousal dimensions. The results
show an accuracy of 0.56 (CI95\%{[}0.46,0.65{]}) for valence and 0.73
(CI95\%{[}0.64,0.81{]}) for arousal recognition.

Vocal automatic emotion recognition is a growing research area in
affective computing. The categorical recognition of emotion remains a
challenge due to the diversity of affective states. However, the
accuracy of a system like SensAI Emotion provides promising results in
the recognition of valence and arousal.

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\hypertarget{ref-banziger2010introducing}{}
Bänziger, T., \& Scherer, K. R. (2010). Introducing the geneva
multimodal emotion portrayal (gemep) corpus. \emph{Blueprint for
Affective Computing: A Sourcebook}, 271--294.

\hypertarget{ref-eyben2018method}{}
Eyben, F., Scherer, K. R., \& Schuller, B. W. (2018). Method for
automatic affective state inference and an automated affective state
inference system. Google Patents.

\hypertarget{ref-poria2017review}{}
Poria, S., Cambria, E., Bajpai, R., \& Hussain, A. (2017). A review of
affective computing: From unimodal analysis to multimodal fusion.
\emph{Information Fusion}, \emph{37}, 98--125.

\end{document}


