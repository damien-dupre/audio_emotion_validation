---
title: "Accuracy of automatic emotion recognition from voice"
author:
  - name: Damien Dupr√©
    email: damien.dupre@ucd.ie
    affiliation: University College Dublin
    footnote: Corresponding Author
  - name: Gary McKeown
    email: g.mckeown@qub.ac.uk
    affiliation: Queen's University Belfast
address:
  - code: University College Dublin
    address: The Insight Centre for Data Analytics, University College Dublin, Republic of Ireland
  - code: Queen's University Belfast
    address: School of Psychology, Queen's University Belfast, United Kingdom

abstract: |

journal: "ISRE 2019"
date: "`r Sys.Date()`"
bibliography: mybibfile.bib
output: rticles::elsevier_article
citation_package: biblatex
csl: apa.csl
---

```{r setup, include = FALSE}
# general options ----------------------------------------------------------------------
options(scipen = 999) #disable sci number format
# chunk options ----------------------------------------------------------------
knitr::opts_chunk$set(
  cache.extra = knitr::rand_seed, 
  message = FALSE, 
  warning = FALSE, 
  error = FALSE, 
  echo = FALSE, 
  fig.align="center"
  )
# libraries --------------------------------------------------------------------
library(papaja)
library(tidyverse)
# data -------------------------------------------------------------------------
raw_data <- readr::read_csv("../../data/audio_emotion_data/audeering_gemep.csv") 
```

```{r category-accuracy}
category_accuracy <- raw_data %>%
  dplyr::filter(emotion %in% c("dis", "con","fea", "irr","anx", "int","joy", "pri","sad", "ang")) %>%
  dplyr::mutate(emotion_c = case_when(
    emotion == "dis" ~ "disgust",
    emotion == "con" ~ "contentment",
    emotion == "fea" ~ "fear",
    emotion == "irr" ~ "irritation",
    emotion == "anx" ~ "worry",
    emotion == "int" ~ "interest",
    emotion == "joy" ~ "joy",
    emotion == "pri" ~ "pride",
    emotion == "sad" ~ "sadness",
    emotion == "ang" ~ "anger"
  )) %>%
  dplyr::mutate(match_emotion = ifelse(emotion_c == segments.emotion.category.name, 1, 0))
```

```{r dimension-accuracy}
VA_accuracy <- raw_data %>%
  dplyr::filter(emotion %in% c("joy", "amu","pri", "ang","fea", "des","ple", "rel","int", "irr","anx", "sad")) %>%
  dplyr::select(name_audio, ppt, emotion,contains("emotion")) %>%
  dplyr::mutate(emotion_valence = case_when(
    emotion %in% c("joy","amu","pri","ple","rel","int") ~ "positive",
    emotion %in% c("ang","fea","des","irr","anx","sad") ~ "negative"
  )) %>%
  dplyr::mutate(emotion_valence_c = case_when(
    emotion %in% c("joy","amu","pri","ple","rel","int") ~ 1,
    emotion %in% c("ang","fea","des","irr","anx","sad") ~ 0
  )) %>%
  dplyr::mutate(emotion_arousal = case_when(
    emotion %in% c("joy", "amu","pri", "ang","fea", "des") ~ "high",
    emotion %in% c("ple", "rel","int", "irr","anx", "sad") ~ "low"
  )) %>%
  dplyr::mutate(emotion_arousal_c = case_when(
    emotion %in% c("joy", "amu","pri", "ang","fea", "des") ~ 1,
    emotion %in% c("ple", "rel","int", "irr","anx", "sad") ~ 0
  )) %>%
  dplyr::group_by(name_audio,emotion, emotion_valence, emotion_arousal) %>%
  dplyr::summarise_if(is.numeric,mean) %>%
  dplyr::mutate(rec_arousal = ifelse(segments.emotion.dimensions.activation>0, 1,0)) %>%
  dplyr::mutate(rec_valence = ifelse(segments.emotion.dimensions.valence>0, 1,0)) %>%
  as.data.frame()

VA_accuracy$emotion_valence_c <- as.factor(as.character(VA_accuracy$emotion_valence_c))
VA_accuracy$emotion_arousal_c <- as.factor(as.character(VA_accuracy$emotion_arousal_c))
VA_accuracy$rec_valence <- as.factor(as.character(VA_accuracy$rec_valence))
VA_accuracy$rec_arousal <- as.factor(as.character(VA_accuracy$rec_arousal))

valence_confusion_matrix <- caret::confusionMatrix(VA_accuracy$emotion_valence_c, VA_accuracy$rec_valence)
arousal_confusion_matrix <- caret::confusionMatrix(VA_accuracy$emotion_arousal_c, VA_accuracy$rec_arousal)
```

Voice is one of the main communicative sources of evidence in interpreting the expression of emotion [@poria2017review]. Affective computing aims to create systems and algorithms that automatically analyse people's emotional state. Consequently, several companies such as Affectiva, Beyond Verbal and Audeering have developed automatic systems to analyse the vocal expression of emotions. However, little is known about the accuracy of such systems.

To evaluate the accuracy of automatic emotion recognition from voice, we processed vocal expressions from the GEMEP database with "SensAI Emotion" developed by Audeering. The GEMEP database contains audio-video recordings from 10 actors performing 17 different emotional scenarios in which they vocalize the same non-words with an emotional tonality [@banziger2010introducing].SensAI Emotion analyses emotions from speech and renders a value for 23 affective states and for valence and arousal dimensions [@eyben2018method].

In terms of category recognition, the accuracy of SensAI labelling GEMEPvocal expressions of emotion is `r scales::percent(sum(category_accuracy$match_emotion)/nrow(category_accuracy))`. However, this low result is partly due to the high number of different affective state labels recognized. To bypass this label matching bias, we compared the recognition accuracy for valence and arousal dimensions. The results show an accuracy of `r round(valence_confusion_matrix$overall["Accuracy"],2)` (CI95%[`r round(valence_confusion_matrix$overall["AccuracyLower"],2)`,`r round(valence_confusion_matrix$overall["AccuracyUpper"],2)`]) for valence and `r round(arousal_confusion_matrix$overall["Accuracy"],2)` (CI95%[`r round(arousal_confusion_matrix$overall["AccuracyLower"],2)`,`r round(arousal_confusion_matrix$overall["AccuracyUpper"],2)`]) for arousal recognition.

Vocal automatic emotion recognition is a growing research area in affective computing. The categorical recognition of emotion remains a challenge due to the diversity of affective states. However, the accuracy of a system like SensAI Emotion provides promising results in the recognition of valence and arousal.

References {#references .unnumbered}
==========